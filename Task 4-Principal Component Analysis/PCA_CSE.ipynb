{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCA-CSE",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgC_aB06rhVt"
      },
      "source": [
        "# Principal Component Analysis and Constant Shift Embedding\n",
        "\n",
        "Güney Işık Tombak \\\\ Bogazici AI\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Il97jNrx8J"
      },
      "source": [
        "## A) Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb7PAqNUsJLv"
      },
      "source": [
        "#### 1) Eigenvalues and Eigenvectors\n",
        "\n",
        "Describe briefly eigenvalues $\\lambda_i$ and eigenvectors $v_i$ of a square matrix $A$. \n",
        "Write down also the mathematical definition in terms of the variables given above. \n",
        "Realize that you should use $\\LaTeX$ notation.\n",
        "\n",
        "Then use `numpy.linalg.eig` to find the eigenvalues and eigenvectors of matrices `A`, `B`, `C`, `D`, and `E`. and print them. Also print the dot product of eigenvectors. What do you observe and what does it mean mathematically? When do we see an eigenvalue of $0$? What is the affect of scalar multiplication on eigenvalues and eigenvectors? What is the affect of scalar addition to diagonal entries on eigenvalues and eigenvectors? Are the eigenvectors unique (other than their negative) for all matrices? Comment on these. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb368ybbuVw7"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySiWLao3rcqQ"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import eig\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "A = np.array([[0, 2], \n",
        "              [2, 3]])\n",
        "\n",
        "B = np.array([[1, 2], \n",
        "              [2, 4]])\n",
        "\n",
        "C = np.array([[0, 0], \n",
        "              [0, 0]])\n",
        "\n",
        "D = 5*A\n",
        "\n",
        "E = A + 5*np.eye(2)\n",
        "\n",
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niLvKJNyxGsQ"
      },
      "source": [
        "#### 2) Principal Component Analysis\n",
        "\n",
        "For the Iris dataset given the cell below, use eigenvalue decomposition to the covariance matrix of the dataset. Then, compare eigenvectors with the SciKit-Learn's PCA results. For the number of components, try all numbers from 1 to 4. By using `%timeit`, compare the duration of PCA runs with different number of components.\n",
        "\n",
        "After this, visualize the samples in 2D and 3D where the targets are depicted as colors. To achieve this, use PCA with 2 and 3 components.\n",
        "\n",
        "Comment on your results. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvGWANTC4oYM"
      },
      "source": [
        "import pandas as pd\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "\n",
        "# load dataset into Pandas DataFrame\n",
        "df = pd.read_csv(url, names=['sepal length','sepal width',\n",
        "                             'petal length','petal width','target'])\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "features = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
        "# Separating out the features\n",
        "x = df.loc[:, features].values\n",
        "# Separating out the target\n",
        "y = df.loc[:,['target']].values\n",
        "# Standardizing the features\n",
        "x = StandardScaler().fit_transform(x)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blMcpTjO5P5k"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEKzhspr9KOG"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV0zd1MAx31k"
      },
      "source": [
        "## B) Constant Shift Embedding\n",
        "\n",
        "#### Prologue \n",
        "\n",
        "In our standard understanding of samples, we try to fit them into an Euclidean (in general, Hilbert) space. However, the data is not given fitting to our everyday understanding of distance, which is \n",
        "\n",
        "$$d(x, y) = \\|x - y\\| = \\sqrt{\\sum_{k=1}^d |x_k - y_k|}.$$\n",
        "\n",
        "More generally, the spaces that we encounter frequently are *metric*, for which the following conditions are satisfied:\n",
        "\n",
        "1.   Identity: $d(x,x) = 0$\n",
        "2.   Non-negativity: $\\forall (x,y): d(x,y) \\geq 0$\n",
        "3.   Symmetry: $d(x,y) = d(y,x)$\n",
        "4.   Triangle Inequality: $d(x,y) \\leq d(x,z) + d(y,z)$\n",
        "\n",
        "In some cases, we might have a matrix of size $n \\times n$ containing the distances between pairwise objects. More interestingly, these distances might not satisfy the metric space conditions. In this case, we can express distances in a so-called dissimilarity matrix $D$ and similarities in an, again so-called similarity matrix $S$. Without a topological embedding, all these relations are can be conceptualized using **Graph Theory**.\n",
        "\n",
        "**All the  parts should be implemented on the similarity matrix computed in the cell below.**\n",
        "\n",
        "If you are not happy with the results, you can try different transforms that you can find in this [link](https://cran.r-project.org/web/packages/linkprediction/vignettes/proxfun.html)\n",
        "\n",
        "References: \n",
        "\n",
        "1.   Main article: [Optimal cluster preserving embedding of nonmetric proximity data](https://ieeexplore.ieee.org/document/1251147)\n",
        "2.   Dataset: [email-Eu-core](https://snap.stanford.edu/data/email-Eu-core.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cCVdpzo-fL1",
        "outputId": "51af69ca-a28c-4a16-c8c2-891eafefe7ee"
      },
      "source": [
        "# Data Read\n",
        "\n",
        "import scipy.linalg as la\n",
        "import numpy as np\n",
        "\n",
        "NUM_NODES = 1005\n",
        "\n",
        "def adj2sim(A, betaMult=0.7):\n",
        "    \n",
        "    # Method: Katz Centrality\n",
        "    # Katz, L. A new status index derived from sociometric analysis. \n",
        "    # Psychometrika 18, 39–43 (1953).\n",
        "    \n",
        "    _, l, _ = la.svd(A)\n",
        "    beta = betaMult/l.max()\n",
        "    I = np.eye(A.shape[0])\n",
        "    S = np.linalg.inv(I - beta*A) - I\n",
        "\n",
        "    return S\n",
        "\n",
        "# initialize data matrix which will be an adjacency matrix\n",
        "DATA = np.zeros((NUM_NODES, NUM_NODES))\n",
        "\n",
        "# fill out the symmetric adjacency matrix\n",
        "with open(\"email-Eu-core.txt\") as file:\n",
        "    for line in file:\n",
        "        pair = [int(x) for x in line.split()]\n",
        "        DATA[pair[0], pair[1]] = 1\n",
        "        DATA[pair[1], pair[0]] = 1\n",
        "\n",
        "S_DATA = adj2sim(DATA)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.34328836e-02 1.13448802e-02 1.55000413e-03 ... 9.93125205e-05\n",
            "  7.12774878e-06 7.04769355e-06]\n",
            " [1.13448802e-02 1.46394427e-02 1.45482701e-03 ... 1.02830753e-04\n",
            "  1.21160727e-05 6.33376050e-06]\n",
            " [1.55000413e-03 1.45482701e-03 2.17420839e-02 ... 1.42356611e-05\n",
            "  2.45589376e-05 1.32850899e-04]\n",
            " ...\n",
            " [9.93125205e-05 1.02830753e-04 1.42356611e-05 ... 8.27330233e-05\n",
            "  1.47160780e-07 6.18092008e-08]\n",
            " [7.12774878e-06 1.21160727e-05 2.45589376e-05 ... 1.47160780e-07\n",
            "  8.35639124e-05 9.95162704e-08]\n",
            " [7.04769355e-06 6.33376050e-06 1.32850899e-04 ... 6.18092008e-08\n",
            "  9.95162704e-08 8.34351012e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeJnBt9OyRsQ"
      },
      "source": [
        "#### 1) Similarity and Dissimilarity\n",
        "\n",
        "Implement the definitions below as functions for similarity $S$ and dissimilarity $D$ matrices. Also answer the questions regarding them.\n",
        "\n",
        "a) Psychological: $S_{ij} = e^{-D_{ij}/\\Delta}$\n",
        "\n",
        "Derive the inverse formula. Using `numpy.allclose` show that the inverse operation is true. What is the effect of $\\Delta$? Comment on the extreme cases $\\Delta = 0$ and  $\\Delta \\rightarrow \\infty$.\n",
        "\n",
        "b) Linear: $S_{ij} = max(D) - D_{ij}$\n",
        "\n",
        "Derive the inverse formula. Using `numpy.allclose` show that the inverse operation is true. What could be done for normalization of this method?\n",
        "\n",
        "c) Euclidean: $D_{ij} = S_{ii} + S_{jj} - 2S_{ij}$\n",
        "\n",
        "Why this one called as Euclidean distance? Please mathematically explain. \n",
        "*Hint:* $S_{ij} \\iff x_i^\\top x_j$\n",
        "\n",
        "**Note:** Any similarity or dissimilarity matrix should be symmetric, i.e. $S_{ij} = S_{ji}$ and $D_{ij} = D_{ji}$. Therefore, implement the symmetrization as a seperate function and use it at the very beginning of the code.\n",
        "\n",
        "$$A^{s}_{ij} = \\frac{A_{ij} + A_{ji}}{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnzqNuyN1ofy"
      },
      "source": [
        "def symmetrize(A):\n",
        "    pass\n",
        "    # Your answer here\n",
        "\n",
        "def psych_D2S(D, del=1):\n",
        "    pass\n",
        "    # Your answer here\n",
        "\n",
        "\n",
        "def psych_S2D(S, del=1):\n",
        "    pass\n",
        "    # Your answer here\n",
        "\n",
        "\n",
        "def linear_D2S(D):\n",
        "    pass \n",
        "    # Your answer here\n",
        "\n",
        "    \n",
        "def linear_S2D(S):\n",
        "    pass \n",
        "    # Your answer here\n",
        "\n",
        "def euc_S2D(S):\n",
        "    pass\n",
        "    # Your answer here\n",
        "\n",
        "\n",
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhGus6_J5bAh"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAwK3Bs55-80"
      },
      "source": [
        "#### 2) Euclidean Projection by Centralization and Shift of Similarity Matrix\n",
        "\n",
        "Explain why the inverse of the Euclidean transform from dissimilarity to similarity is not unique under the definition: $D_{ij} = S_{ii} + S_{jj} - 2S_{ij}$.\n",
        "\n",
        "One way to achieve uniqueness is using centralization, which is defined for any matrix $V$:\n",
        "\n",
        "$$A^c = QAQ^\\top, \\ \\ \\ Q = I_n - \\frac{1_n}{n}$$ where $I_n$ is identity matrix whereas $1_n$ is all 1 matrix, both of size $n \\times n$. \n",
        "\n",
        "With a series of calculations we achieve\n",
        "\n",
        "$$S^c_{ij} = - \\frac{1}{2} D^c_{ij}$$\n",
        "\n",
        "and with this we have a unique definition for both sides. \n",
        "\n",
        "Now, we can consider $S$ as a covariance matrix such that $S = XX^\\top$. Hence, we can represent the $n$ objects in these matrices in an Euclidean space. To achieve these we can use Singular Value Decomposition such that \n",
        "\n",
        "$$S = V \\Lambda V^\\top \\iff X = V \\Lambda^\\frac{1}{2}.$$\n",
        "\n",
        "Realize that to achieve vectors in Euclidean space, the diagonal matrix $\\Lambda$ should be all positive. To make all eigenvalues greater or equal than zero ($\\lambda_i \\geq 0$), we can subtract the lowest eigenvalue of $S$ from $S$ diagonally.\n",
        "\n",
        "$$\\tilde{S} = S - \\lambda_{min}(S).$$\n",
        "\n",
        "In this part, define a class that contains $D^c$, $S^c$, $\\tilde{S}$, and $X$. The class should be initializable using a $D$ or $C$. The steps are:\n",
        "\n",
        "$D \\rightarrow D^s \\rightarrow D^c \\rightarrow S^c \\rightarrow \\tilde{S} \\rightarrow X$ for starting from a $D$. If S is given, use the standard formula first:  $D_{ij} = S_{ii} + S_{jj} - 2S_{ij}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCb-e2bh7YXe"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91IOPIbq5dQj"
      },
      "source": [
        "class Euclidean_Projection():\n",
        "    def __init__(self, matrix, matrix_type='D'):\n",
        "        if matrix_type.lower() == 'd':\n",
        "            D = matrix\n",
        "        else:\n",
        "            D = self.s2d(matrix)\n",
        "\n",
        "        D_s = self.symmetrize(D)\n",
        "        D_c = self.center(D_s)\n",
        "        S_c = self.d_c2s_c(D_c)\n",
        "        S_t = self.diag_shift(S_c)\n",
        "        X = self.s2x(S_t)\n",
        "\n",
        "        self.D_c = D_c\n",
        "        self.S_c = S_c\n",
        "        self.S_t = S_t\n",
        "        self.X = X\n",
        "\n",
        "        # Your answer here\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b98IwJYLsCMe"
      },
      "source": [
        "#### 3) Dimension Trim\n",
        "\n",
        "Realize that although we can project the objects into an Euclidean space, the space has to have dimension of number of objects, since $X$ is an $n \\times n$ matrix. We can change this using the first $p \\leq n$ principal components of $\\tilde{S}$. Realize that the eigenvalues are given in descending order, i.e. $\\forall \\lambda, \\lambda_{i+1} \\leq \\lambda_i$.\n",
        "\n",
        "Therefore, instead of\n",
        "\n",
        "$$X = V \\Lambda^\\frac{1}{2} =\\begin{bmatrix}\n",
        "    \\vert & \\vert & \\vert \\\\\n",
        "    v_1   & ...  & v_n \\\\\n",
        "    \\vert & \\vert & \\vert\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "    \\lambda_1^\\frac{1}{2} & 0 & 0 \\\\\n",
        "    0   & ...  & 0 \\\\\n",
        "    0 & 0 & \\lambda_n^\\frac{1}{2}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "we can do \n",
        "\n",
        "$$X_p = V_p \\Lambda_p^\\frac{1}{2} =\\begin{bmatrix}\n",
        "    \\vert & \\vert & \\vert \\\\\n",
        "    v_1   & ...  & v_p \\\\\n",
        "    \\vert & \\vert & \\vert\n",
        "\\end{bmatrix}_{n \\times p} \\begin{bmatrix}\n",
        "    \\lambda_1^\\frac{1}{2} & 0 & 0 \\\\\n",
        "    0   & ...  & 0 \\\\\n",
        "    0 & 0 & \\lambda_p^\\frac{1}{2} \\\\\n",
        "\\end{bmatrix}_{p \\times p}.$$\n",
        "\n",
        "Please copy your work from the previous step and write another class method namely `s2x_p(S_t, p=None)`. If $p$ is not given, the code should give the standard $X$ (i.e. $p = n$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM36-4pyz4Bl"
      },
      "source": [
        "class Constant_Shift_Embedding():\n",
        "    \n",
        "    # Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzltiMhE0GtP"
      },
      "source": [
        "#### 4) Visualization and Clustering\n",
        "\n",
        "Determine the variable $p$ as small as possible satisfying that $\\sum_{k=1}^p \\lambda_p \\geq \\frac{9}{10}\n",
        "\\sum_{k=1}^n \\lambda_k$. Then using CSE class you previously written, determine the $X_p$ and print the corresponding $D_p$ as a heatmap. Do the same for $p = 2$ and $p = 3$ and also show $X_{p=2}$ $X_{p=2}$ on Euclidean spaces (using scatter). You can use Matplotlib, Seaborn, or any other visualization tool.\n",
        "\n",
        "After these, use K-Means of Sci-Kit Learn to cluster the samples. Use $K = 1:10$ and determine the number of clusters according to the elbow method.\n",
        "\n",
        "Comment on your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjqKJtX2emGQ"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdjr7JUneoFg"
      },
      "source": [
        "Your answer here"
      ]
    }
  ]
}