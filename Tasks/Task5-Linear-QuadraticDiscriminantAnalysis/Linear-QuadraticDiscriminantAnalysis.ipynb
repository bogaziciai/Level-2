{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA-QDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeWWuVc29OgB"
      },
      "source": [
        "# Linear Discriminant Analysis & Quadratic Discriminant Analysis\n",
        "\n",
        "Güney Işık Tombak \\\\ Bogazici AI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS7xxKm_WgTY"
      },
      "source": [
        "**Important Note:**\n",
        "\n",
        "*Parts A and B are optional but recommended*, **C and D must be done.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaPEn80o9KZA"
      },
      "source": [
        "# Standard Imports\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "\n",
        "# use seaborn plotting defaults\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [8, 6]\n",
        "\n",
        "random_state = 42 # Check Hitchhiker's Guide to the Galaxy \n",
        "# Do Not Change It "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dog2v0LD9n6e"
      },
      "source": [
        "## A) Prologue: Linear Dichotomies (Optional)\n",
        "\n",
        "For a set of $n$ points *in general position* inside a $d$ dimensional space $(\\{\\mathbf{x}\\}_{i=1}^n \\in \\mathbb{R^d})$, there are\n",
        "\n",
        "$$C(n,d) = 2 \\sum_{i=0}^{d-1} {n-1 \\choose i} = 2 \\sum_{i=0}^{d-1} \\frac{(n-1)!}{i!(n-1-i)!}$$\n",
        "\n",
        "possible linear dichotomies. This is known as [**Cover's Theorem**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4038449&casa_token=mff1wM7fQSMAAAAA:KbkambCYJAP_wifALgrvYsgGA0KQh90hev-7ettzoyauMcxMm4UmZS7GMLWmpS0Qlkv4pzZVjg&tag=1).\n",
        "\n",
        "You should realize that the seperative hyperplane $(\\mathbb{R}^{d-1})$ should cross the origin to linear creation of dichotomies. \n",
        "\n",
        "[General position](https://en.wikipedia.org/wiki/General_position): Any combination of $k$ samples should not lie in a flat of $\\mathbb{R}^{k-2}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLAgNq_vtzIU"
      },
      "source": [
        "#### 1) Implementation of Cover's Theorem\n",
        "\n",
        "Implement the Cover's theorem as a function for a given $(n,d)$. Moreover, using the recursive relation, create a recursive version of the algorithm:\n",
        "\n",
        "$$C(n+1,d) = C(n,d) + C(n, d-1).$$\n",
        "\n",
        "For large $n$ and $d$ values, compare the computational duration of these functions (use `%timeit`).\n",
        "\n",
        "Hint: Realize $C(1,m) = \\begin{cases} 2 & m \\geq 1 \\\\ 0 & m < 1 \\end{cases}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8S2qNjJEFAG"
      },
      "source": [
        "def covers_theorem(n, d):\n",
        "    pass\n",
        "    # Your code here\n",
        "\n",
        "\n",
        "def recursive_covers_theorem(n, d):\n",
        "    pass\n",
        "    # Your code here\n",
        "\n",
        "\n",
        "#n, d = SOME_BIG_NUMBER_1, SOME_BIG_NUMBER_2\n",
        "\n",
        "# %timeit covers_theorem(n, d)\n",
        "# %timeit recursive_covers_theorem(n, d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeuqHuVNEQL-"
      },
      "source": [
        "#### 2) 2D Example\n",
        "\n",
        "Check the 2D datasets below. Calculate the number of possible linear dichotomies and show an example of them on the dataset as a line. Realize that you have to determine only one variable ($a$) since the equation is $y = ax$. Since $a$ should be able to equal to $\\pm \\infty$, we can use\n",
        "\n",
        "$$y = x \\cdot tan(\\theta)$$\n",
        "\n",
        "where $\\theta$ is defined in radians (i.e. $\\pi = 180^{\\circ}$). You can use [`numpy.deg2rad`](https://numpy.org/doc/stable/reference/generated/numpy.deg2rad.html).\n",
        "\n",
        "Are the results consistent? If not, please explain why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sPVjnQrEca0"
      },
      "source": [
        "angle_trunc = lambda a: (a+np.ceil(np.abs(a)/(2*np.pi))*2*np.pi+np.pi)%(np.pi*2) - np.pi\n",
        "\n",
        "def scat(X, theta=None, title=None):\n",
        "\n",
        "    coor_max = np.max(np.abs(X))\n",
        "\n",
        "    plt.scatter(X[:,0], X[:,1])\n",
        "    plt.plot([-coor_max, coor_max], [0,0], '-k', linewidth=2)\n",
        "    plt.plot([0,0], [-coor_max, coor_max], '-k', linewidth=2)\n",
        "\n",
        "    if theta is not None:\n",
        "        theta = angle_trunc(theta)\n",
        "        x = np.array([-np.cos(theta), np.cos(theta)])\n",
        "        y = np.array([-np.sin(theta), np.sin(theta)])\n",
        "        c1 = np.abs(coor_max / np.sin(theta))\n",
        "        c2 = np.abs(coor_max / np.cos(theta))\n",
        "        l = min(c1, c2)\n",
        "        plt.plot(l*x, l*y, '--r', linewidth=2)\n",
        "\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "\n",
        "# Dataset 1\n",
        "X1 = [[1, 1], [-1, -2], [1, -1]]\n",
        "X1 = np.asarray(X1)\n",
        "\n",
        "plt.figure(1)\n",
        "scat(X1, title='First Dataset')\n",
        "plt.show()\n",
        "\n",
        "#Dataset 2\n",
        "X2 = [[-0.5, -1], [-1.5, 1], [1, 2]]\n",
        "X2 = np.asarray(X2)\n",
        "\n",
        "plt.figure(2)\n",
        "scat(X2, title='Second Dataset')\n",
        "plt.show()\n",
        "\n",
        "# Example of how to use scat:\n",
        "\n",
        "angle = np.pi/5\n",
        "plt.figure(3)\n",
        "scat(X2, theta=angle, title='Example')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odp43f4ri75m"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oCsnG9aE6iN"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMTXYiVuExvN"
      },
      "source": [
        "#### 3) Automatic 2D Possible Number of Dichotomies Detection\n",
        "\n",
        "Find a way to automatically detect number of possible dichotomies using a brute-force technique (try all *???*). Realize that for each point $\\mathbf{x}$, you can detect its class assignment using the equation below:\n",
        "\n",
        "$$sign(\\mathbf{w}^\\top \\mathbf{x})$$\n",
        "\n",
        "Explain your reasoning briefly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2-hppUapA5N"
      },
      "source": [
        "def find_n_dichotomies(X):\n",
        "    pass\n",
        "    # Your answer here\n",
        "\n",
        "print(find_n_dichotomies(X1))\n",
        "print(find_n_dichotomies(X2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du-wM0UkpkY3"
      },
      "source": [
        "Your answer here "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3oRKjIFqCML"
      },
      "source": [
        "## B) Separation with Lines (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w8_ut4Npauz"
      },
      "source": [
        "# Toy Dataset Creation\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=random_state, cluster_std=2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='jet')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnKNRxvctrig"
      },
      "source": [
        "#### 1) Possible Lines\n",
        "\n",
        "Create at least three different lines to separate the clusters described above\n",
        "\n",
        "$x_2 = mx_1 + b$\n",
        "\n",
        "Format: `[(m_1, b_1), (m_2, b_2), ..., (m_N, b_N)]`\n",
        "\n",
        "Tip: You can use Desmos or GeoGebra to determine the line more easily"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n0muaLXtoB5"
      },
      "source": [
        "mb = # Your answer here\n",
        "\n",
        "xfit = np.linspace(-10, 10) \n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='jet')\n",
        "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
        "\n",
        "for m, b in mb:\n",
        "    plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10A2xsBRuVeu"
      },
      "source": [
        "#### 2) Combining Clusters\n",
        "\n",
        "Using the mean and variance of the all samples \n",
        "create a Gaussian random dataset of 1000 points.\n",
        "\n",
        "Use the random_state provided and plot them\n",
        "\n",
        "Note: Both variance and the mean is in 2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFuyw67suio9"
      },
      "source": [
        "def gauss_randomizer(X_in, random_state=random_state):\n",
        "    \n",
        "    N = 1000\n",
        "    mean_X, std_X = np.mean(X_in), np.std(X_in)\n",
        "\n",
        "    # Your answer here\n",
        "\n",
        "    return X_out\n",
        "\n",
        "Xg = gauss_randomizer(X) \n",
        "\n",
        "plt.scatter(Xg[:, 0], Xg[:, 1], s=50, cmap='jet')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrgL3SIlul4u"
      },
      "source": [
        "#### 3) Weights\n",
        "\n",
        "Derive the vector $\\mathbf{w}$ in terms of m and b realizing that the line equation can also be written as \n",
        "\n",
        "$\\mathbf{w} \\cdot \\mathbf{x} = c$\n",
        "\n",
        "Divide the samples into two clusters using the lines you determined in `mb`\n",
        "\n",
        "Show the number of points in each cluster for each line using the equation:\n",
        "\n",
        "$\\hat{y} = \\begin{cases} -1 & \\mathbf{w} \\cdot \\mathbf{x} - c < 0 \\\\ +1 & \\mathbf{w} \\cdot \\mathbf{x} - c \\geq 0 \\end{cases}$\n",
        "\n",
        "The alternative formulation of the equation is:\n",
        "\n",
        "$\\hat{y} (\\mathbf{w} \\cdot \\mathbf{x} - c) \\geq 0$\n",
        "\n",
        "Comment on the results you achieved. Which line seems to be better? Why? Can you name two parameters of the initial distributions that you consider when you fitting the line?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp1jpjXuutkQ"
      },
      "source": [
        "xfit = np.linspace(-10, 10)\n",
        "\n",
        "# Your answer here\n",
        "\n",
        "for m, b in mb:\n",
        "    plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "    # and here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coRIwwqEuxpb"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j693cS3UHsA5"
      },
      "source": [
        "## C) Linear Discriminant Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WoDPKUkH8vF"
      },
      "source": [
        "# Toy Dataset Creation\n",
        "\n",
        "def toy_dataset_create(n_samples=100, show_cls=True):\n",
        "\n",
        "    X_list, y_list = list(), list()\n",
        "    std_list = [3,2,4]\n",
        "    rand_sts = [2,3,10]\n",
        "\n",
        "    for cls_std, rand_state in zip(std_list, rand_sts):\n",
        "        X, y = make_blobs(n_samples=n_samples, centers=2,\n",
        "                        random_state=rand_state, cluster_std=cls_std)\n",
        "        \n",
        "        X_list.append(X)\n",
        "        y_list.append(y)\n",
        "\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='jet')\n",
        "        if show_cls:\n",
        "            plt.show()\n",
        "\n",
        "    return X_list, y_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3b-WlzgH-oa"
      },
      "source": [
        "X_list, y_list = toy_dataset_create()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBxSa4ceHwdx"
      },
      "source": [
        "#### 1) Mean Divider\n",
        "\n",
        "To divide a cluster into two, we can consider the mean of each distribution and think a hypothetical vector going from one mean to the other. Then we can divide the distribution into clusters by a line crossing the midpoint of the means and orthogonal to the vector.\n",
        "\n",
        "Prove that this hypothetical vector is indeed $w$. Then, for each distribution above, plot the $w$ and the division line $l$ using the midpoint $\\mathbf{\\mu}$.\n",
        "\n",
        "Realize that $w \\cdot l = 0$ due to orthogonality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv3F7HmeOD9N"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHHU0L9oOGxw"
      },
      "source": [
        "for X, y in zip(X_list, y_list):\n",
        "\n",
        "    # Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk5AoQm4OFwR"
      },
      "source": [
        "#### 2) Effect of Number of Samples\n",
        "\n",
        "Do the same for different number of samples. Do you observe any change? Please discuss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edQJZaclOZ1Q"
      },
      "source": [
        "n_samples_list = [10, 100, 1000]\n",
        "\n",
        "for n_samples in n_samples_list:\n",
        "\n",
        "    X_list, y_list = toy_dataset_create(n_samples=n_samples, \n",
        "                                        show_cls=False)\n",
        "\n",
        "    for X, y in zip(X_list, y_list):\n",
        "\n",
        "        # Your answer here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWfEIBm3Obk4"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9IYW2fROfAi"
      },
      "source": [
        "#### 3) LDA of Sci-Kit Learn\n",
        "\n",
        "Use Linear Discriminant Analysis (LDA) by Sci-kit learn to do the same at 2b. \n",
        "\n",
        "To visualize the decision boundary, use $w=$ `clf.coef_` and $c=$ `clf.intercept_` .\n",
        "\n",
        "What do you observe? Is it similar to the previous results? What LDA considers other than the mean of the clusters? What might be a problem for LDA?\n",
        "\n",
        "*Hint: [LDA and QDA by Scikit Learn](https://scikit-learn.org/stable/modules/lda_qda.html)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSFefKaJOqb1"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "\n",
        "clf = LDA()\n",
        "n_samples_list = [10, 100, 1000]\n",
        "\n",
        "for n_samples in n_samples_list:\n",
        "\n",
        "    X_list, y_list = toy_dataset_create(n_samples=n_samples, \n",
        "                                        show_cls=False)\n",
        "\n",
        "    for X, y in zip(X_list, y_list):\n",
        "\n",
        "        # Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDckTYhWOsiA"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il7f5p_0OwMG"
      },
      "source": [
        "#### 4) Difficulties for LDA\n",
        "\n",
        "Construct two distributions that geometrically meaningful (a human can divide by looking at it easily) and LDA fails. Explain why LDA fails and propose a solution (no need for an implementation).\n",
        "\n",
        "*Hint: You can use `numpy.random.multivariate_normal`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRS0z89QO6eH"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOCHJS_bO8M5"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR7FL6kEP_Wr"
      },
      "source": [
        "## D) Quadratic Discriminant Analysis\n",
        "\n",
        "Try part 4 and 5 using Quadratic Discriminant Analysis. The main difference is, instead of using $\\mathbf{w}^\\top \\mathbf{x} + w_0 = 0$, QDA looks for a more complex set of weights, such that:\n",
        "\n",
        "$$\\mathbf{x}^\\top W_2 \\mathbf{x} + \\mathbf{w}_1^\\top \\mathbf{x} + w_0 = 0$$\n",
        "\n",
        "Comment on the differences you observed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHsG9NPvQt7Q"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
        "\n",
        "clf = QDA()\n",
        "n_samples_list = [10, 100, 1000]\n",
        "\n",
        "for n_samples in n_samples_list:\n",
        "\n",
        "    X_list, y_list = toy_dataset_create(n_samples=n_samples, \n",
        "                                        show_cls=False)\n",
        "\n",
        "    for X, y in zip(X_list, y_list):\n",
        "\n",
        "        # Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JVObtIEQ2zX"
      },
      "source": [
        "Your answer here"
      ]
    }
  ]
}
