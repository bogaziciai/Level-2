{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSNcEJft3zL0"
      },
      "source": [
        "# Support Vector Machines\n",
        "\n",
        "### Boğaziçi AI\n",
        "\n",
        "by Güney Işık Tombak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNyYKrTo4BXm"
      },
      "source": [
        "###1) Interactive Demo \n",
        "\n",
        "Go to the website below and playing with the interactive SVM answer the questions:\n",
        "\n",
        "[SVM Demo](https://jgreitemann.github.io/svm-demo)\n",
        "\n",
        "1.   Sort from simplest to the most complex: Radial Basis Function, Linear, Quadratic. \n",
        "\n",
        "Bonus: Mathematically prove. *Hint: Derivation of exponential using limit.*\n",
        "2.   Which one is more stable? Higher or lower $\\nu$? Why? What is the relation between $\\nu$ and number of support vectors? What should be the value of $\\nu$ for hard-margin SVM?\n",
        "3.   What do you observe when you change the $\\gamma$ term using Radial Basis Function? Which one is more stable? Higher or lower $\\gamma$? Why? Again, explain with your observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfFCdXk34R74"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IcrQi3BL_6S"
      },
      "source": [
        "###2) C Value\n",
        "\n",
        "The mathematical formulation of a Soft-Margin SVM is:\n",
        "\n",
        "$$min_{\\mathbf{w},c,K} \\frac{1}{2}\\| w\\|^2 + K \\sum_{i=1}^{N} \\epsilon^{(i)}$$\n",
        "\n",
        "$$\\text{subject to } y^{(i)}(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} - c) \\geq 1 - \\epsilon^{(i)} \\text{ and } \\epsilon^{(i)} \\geq 0$$ \n",
        "\n",
        "Realize that the first term of the equation is the regularization term. This means that the errors are punished by the second term. \n",
        "\n",
        "Run the code below. Regarding the results, explain the relation between $\\nu$, $K$, and `C` (e.g. when A goes up, also B goes up). Then, do the same for at least 5 different datasets and \n",
        "comment on the new results.\n",
        "\n",
        "*Note: Original Decision Function Plot from [Python Data Science Handbook](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb) with MIT license.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP44HT--3pDL"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.utils import check_random_state as sk_rand_seed\n",
        "\n",
        "random_state = 42\n",
        "np.random.seed(random_state)\n",
        "sk_rand_seed(random_state)\n",
        "\n",
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "    \n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "    \n",
        "    # plot support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=300, linewidth=1, facecolors='none');\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFflnzuMp3a"
      },
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "\n",
        "C_list = [1, 0.1, 0.01, 0.001]\n",
        "\n",
        "for C in C_list:\n",
        "\n",
        "    model = SVC(kernel='linear', C=C, probability=True)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='jet')\n",
        "    plot_svc_decision_function(model);\n",
        "\n",
        "    plt.title(f'C={C}')\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxBa8RjKNeHh"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdDEmFe6Q-D9"
      },
      "source": [
        "###3) Multiple Classifiers by Dichotomous Ones\n",
        "\n",
        "When you have more than two classes, you can use various techniques to use SVMs. One basic one is using one dichotomous (two-class classifier) SVM for each class.\n",
        "\n",
        "For this exercise, first check how Scikit Learn team used SVM classifiers to classify Iris Dataset [[1]](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html).\n",
        "\n",
        "Then, rather than using SVM as a multiclass classifier, create a dataset for each class and train each dichotomous SVM with probability. You can achieve this by  \n",
        "\n",
        "```\n",
        "clf = svm.SVC(kernel='linear', C=C, probability=True)\n",
        "clf.fit(X,y)\n",
        "clf.predict_proba(X)\n",
        "```\n",
        "\n",
        "Using the probability results from all three, determine the class. You should implement this as a custom sklearn estimator. You can find the details for custom sklearn estimators [[2]](https://scikit-learn.org/stable/developers/develop.html).\n",
        "\n",
        "Use `kernel` types `linear`, `rbf`, quadratic, and cubic (i.e. `poly` `degree`=2 and 3) with `C` values `[1, 0.1, 0.01, 0.001]`.\n",
        "\n",
        "Plot each result using the visualization given in [[1]](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html) into a 4-by-4 grid and give the f1 score (`sklearn.metrics.f1_score` ) of each algorithm on the title along with C value and kernel type.\n",
        "\n",
        "Similar to Scikit Learn, use only Sepal Width and Sepal Length for the easiness of visualization. Comment on your results in the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHmk30IbNM27"
      },
      "source": [
        "# Your answer here\n",
        "# The one below is an unfinished class\n",
        "# It might have some errors so please use with caution!\n",
        "# You can use it as a starting point or directly start from scratch\n",
        "\n",
        "import numpy as np\n",
        "import sklearn as skl\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "class dichSVC(skl.base.BaseEstimator, skl.base.TransformerMixin):\n",
        "\n",
        "    def __init__(self, svc_params_dict,  n_classes=2):\n",
        "        \n",
        "        svc_params_dict['probability'] = True\n",
        "\n",
        "        if n_classes == 2:\n",
        "            self.n_classifiers = 1\n",
        "            self.base_estimator_list = [SVC(**svc_params_dict)]\n",
        "        elif n_classes > 2:\n",
        "            self.n_classifiers = n_classes\n",
        "            self.base_estimator_list = list()\n",
        "            for _ in range(n_classes):\n",
        "                self.base_estimator_list.append(SVC(**svc_params_dict))    \n",
        "\n",
        "    def fit(self, samples, targets):\n",
        "        \n",
        "        for class_id in range(self.n_classifiers):\n",
        "            dich_targets = self.dichotomize_targets(targets, class_id)\n",
        "            self.base_estimator_list[class_id].fit(samples, dich_targets)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def dichotomize_targets(self, targets, class_id):\n",
        "        pass\n",
        "\n",
        "    def predict_proba(self, samples):\n",
        "\n",
        "        n_samples = samples.shape[0]\n",
        "        probs = np.zeros((self.n_classifiers, n_samples))\n",
        "\n",
        "        for class_id in range(self.n_classifiers):\n",
        "            prob_i = self.base_estimator_list[class_id].predict_proba(samples)\n",
        "            probs[class_id, :] = prob_i[:,0] # or might be 1, check!\n",
        "\n",
        "        return probs\n",
        "        \n",
        "    def predict(self, samples):\n",
        "        # You might want to use self.predict_proba here\n",
        "        pass \n",
        "    \n",
        "    def fit_predict(self, samples, targets):\n",
        "        \n",
        "        self.fit(samples, targets)\n",
        "        preds = self.predict(samples)\n",
        "        \n",
        "        return preds\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BF1QiwqTNVr"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDf_-aFZaFHz"
      },
      "source": [
        "Your answer here"
      ]
    }
  ]
}
